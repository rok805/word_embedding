{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from gensim.models import FastText\n",
    "from tqdm import tqdm\n",
    "from soynlp.hangle import compose, decompose, character_is_korean\n",
    "import re\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 844013/844013 [00:27<00:00, 30429.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# 자모를 분해한 파일 생성하기1(말뭉치(corpus) 데이터 불러오기)\n",
    "\n",
    "nouns = []\n",
    "with open('corpus_nouns.txt', 'r', encoding='utf-8') as f:\n",
    "    for sentence in tqdm(f.readlines()):\n",
    "        nouns.append(sentence.split())\n",
    "\n",
    "# morphs = []\n",
    "# with open('/storage/rok/Korean_Embedding/corpus_morphs.txt', 'r') as f:\n",
    "#     for sentence in tqdm(f.readlines()):\n",
    "#         morphs.append(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nouns\n",
    "del morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doublespace_pattern = re.compile('\\s+')\n",
    "def jamo_sentence(sent):\n",
    "\n",
    "    def transform(char):\n",
    "        if char == ' ':\n",
    "            return char\n",
    "        cjj = decompose(char)\n",
    "        if len(cjj) == 1:\n",
    "            return cjj\n",
    "        cjj_ = ''.join(c if c != ' ' else '-' for c in cjj)\n",
    "        return cjj_\n",
    "\n",
    "    sent_ = []\n",
    "    for char in sent:\n",
    "        if character_is_korean(char):\n",
    "            sent_.append(transform(char))\n",
    "        else:\n",
    "            sent_.append(char)\n",
    "    sent_ = doublespace_pattern.sub(' ', ''.join(sent_))\n",
    "    return sent_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jamo_to_word(jamo):\n",
    "    jamo_list, idx = [], 0\n",
    "    while idx < len(jamo):\n",
    "        if not character_is_korean(jamo[idx]):\n",
    "            jamo_list.append(jamo[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            jamo_list.append(jamo[idx:idx+3])\n",
    "            idx += 3\n",
    "    word = \"\"\n",
    "    for jamo_char in jamo_list:\n",
    "        if len(jamo_char) == 1:\n",
    "            word += jamo_char\n",
    "        elif jamo_char[2] == \"-\":\n",
    "            word += compose(jamo_char[0], jamo_char[1], \" \")\n",
    "        else:\n",
    "            word += compose(jamo_char[0], jamo_char[1], jamo_char[2])\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jamo(tokenized_corpus_fname, output_fname):\n",
    "    with open(tokenized_corpus_fname, 'r', encoding='utf-8') as f1, \\\n",
    "            open(output_fname, 'w', encoding='utf-8') as f2:\n",
    "        for line in tqdm(f1):\n",
    "            sentence = line.replace('\\n', '').strip()\n",
    "            processed_sentence = jamo_sentence(sentence)\n",
    "            f2.writelines(processed_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(list):\n",
    "    return [(jamo_to_word(w), r) for(w,r) in list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "844013it [17:57, 783.53it/s] \n"
     ]
    }
   ],
   "source": [
    "# 자모를 분해한 파일 생성하기2 (자모 분해하기: 30분 소요)\n",
    "\n",
    "process_jamo(tokenized_corpus_fname='corpus_nouns.txt', output_fname='corpus_nouns_jamo.txt')\n",
    "\n",
    "# process_jamo(tokenized_corpus_fname='corpus_morphs.txt', output_fname='corpus_morphs_jamo.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 844013/844013 [00:33<00:00, 25374.98it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_morphs = [i.strip().split(\" \") for i in tqdm(open('/storage/rok/Korean_Embedding/corpus_morphs_jamo.txt', 'r', encoding='utf-8').readlines())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 844013/844013 [01:29<00:00, 9428.24it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_nouns = [i.strip().split(\" \") for i in tqdm(open('corpus_nouns_jamo.txt', 'r', encoding='utf-8').readlines())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using skip-gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin=datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fasttext_morphs = FastText(corpus_morphs, vector_size=100, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "end=datetime.now() - begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:08:04.985114\n"
     ]
    }
   ],
   "source": [
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('감염증', 0.9166404604911804),\n",
       " ('신종', 0.8719736933708191),\n",
       " ('바이러스', 0.8613077402114868),\n",
       " ('대유행', 0.8116739988327026),\n",
       " ('코로나도', 0.7756314277648926),\n",
       " ('사이러스', 0.7590113282203674),\n",
       " ('확산', 0.7508911490440369),\n",
       " ('바이러', 0.7111417651176453),\n",
       " ('방역', 0.7084428071975708),\n",
       " ('팬데', 0.7076288461685181)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(fasttext_morphs.wv.most_similar(jamo_sentence('코로나'), topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_morphs.save('/storage/rok/Korean_Embedding/embedding_models/FastText_morphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin=datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_nouns = FastText(corpus_nouns, size=100, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "end=datetime.now() - begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:46:06.539925\n"
     ]
    }
   ],
   "source": [
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('감염증', 0.934538722038269),\n",
       " ('바이러스', 0.8809232711791992),\n",
       " ('신종', 0.8769291639328003),\n",
       " ('대유행', 0.8149067163467407),\n",
       " ('확산', 0.7848906517028809),\n",
       " ('사이러스', 0.7675048112869263),\n",
       " ('바이러', 0.7502391934394836),\n",
       " ('코로나도', 0.7405038475990295),\n",
       " ('확진', 0.7358541488647461),\n",
       " ('여파', 0.7154252529144287)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(fasttext_nouns.wv.most_similar(jamo_sentence('코로나'), topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_nouns.save('FastText_nouns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del corpus_nouns, fasttext_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = gensim.models.FastText.load('FastText_morphs')\n",
    "transform(m0.wv.most_similar(jamo_sentence('코로나'), topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('감염증', 0.9374035000801086),\n",
       " ('신종', 0.8863914608955383),\n",
       " ('바이러스', 0.8838632702827454),\n",
       " ('대유행', 0.813446581363678),\n",
       " ('확산', 0.7794215679168701),\n",
       " ('사이러스', 0.7732242345809937),\n",
       " ('바이러', 0.7599300742149353),\n",
       " ('코로나도', 0.752407968044281),\n",
       " ('확진', 0.7384893894195557),\n",
       " ('여파', 0.7216395139694214)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = gensim.models.FastText.load('FastText_nouns')\n",
    "transform(m1.wv.most_similar(jamo_sentence('코로나'), topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('감염증', 0.934538722038269),\n",
       " ('바이러스', 0.8809232711791992),\n",
       " ('신종', 0.8769291639328003),\n",
       " ('대유행', 0.8149067163467407),\n",
       " ('확산', 0.7848906517028809),\n",
       " ('사이러스', 0.7675048112869263),\n",
       " ('바이러', 0.7502391934394836),\n",
       " ('코로나도', 0.7405038475990295),\n",
       " ('확진', 0.7358541488647461),\n",
       " ('여파', 0.7154252529144287)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = gensim.models.FastText.load('FastText_nouns')\n",
    "transform(m1.wv.most_similar(jamo_sentence('코로나'), topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('감염증', 0.934538722038269),\n",
       " ('바이러스', 0.8809232711791992),\n",
       " ('신종', 0.8769291639328003),\n",
       " ('대유행', 0.8149067163467407),\n",
       " ('확산', 0.7848906517028809),\n",
       " ('사이러스', 0.7675048112869263),\n",
       " ('바이러', 0.7502391934394836),\n",
       " ('코로나도', 0.7405038475990295),\n",
       " ('확진', 0.7358541488647461),\n",
       " ('여파', 0.7154252529144287)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = gensim.models.FastText.load('FastText_nouns')\n",
    "transform(m1.wv.most_similar(jamo_sentence('코로나'), topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 844013/844013 [00:18<00:00, 44727.56it/s]\n"
     ]
    }
   ],
   "source": [
    "nouns = []\n",
    "with open('corpus_nouns.txt', 'r', encoding='utf-8') as f:\n",
    "    for sentence in tqdm(f.readlines()):\n",
    "        nouns.append(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v(data, sg, name):\n",
    "    \n",
    "    begin = datetime.now()\n",
    "    model = Word2Vec(sentences=data, size=100, window=5, min_count=5, workers=4, sg=sg)\n",
    "    end = datetime.now() - begin\n",
    "    print(end)\n",
    "        \n",
    "#     if sg == 0:    \n",
    "#         model.save('CBOW_'+name)\n",
    "#     elif sg == 1:\n",
    "#         model.save('Skip_gram_'+name)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:20:26.536875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('감염증', 0.9315603971481323),\n",
       " ('신종', 0.8666401505470276),\n",
       " ('바이러스', 0.8529074192047119),\n",
       " ('대유행', 0.7721904516220093),\n",
       " ('확산', 0.7620081901550293),\n",
       " ('전별', 0.7346984148025513),\n",
       " ('점종', 0.7079029679298401),\n",
       " ('확진', 0.7045122385025024),\n",
       " ('다립', 0.6960322260856628),\n",
       " ('역선', 0.6932239532470703)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram_nouns = w2v(data=nouns, sg=1, name='nouns')\n",
    "skipgram_nouns.wv.most_similar('코로나', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_nouns.save('Skip_gram_nouns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = gensim.models.Word2Vec.load('Skip_gram_nouns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('감염증', 0.9315603971481323),\n",
       " ('신종', 0.8666401505470276),\n",
       " ('바이러스', 0.8529074192047119),\n",
       " ('대유행', 0.7721904516220093),\n",
       " ('확산', 0.7620081901550293),\n",
       " ('전별', 0.7346984148025513),\n",
       " ('점종', 0.7079029679298401),\n",
       " ('확진', 0.7045122385025024),\n",
       " ('다립', 0.6960322260856628),\n",
       " ('역선', 0.6932239532470703)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.wv.most_similar('코로나', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ㅇㅓㅁㅍㅖ-ㅁㅜㄹ', 0.3091914653778076),\n",
       " ('ㅅㅜ-ㄹㅠㄱ', 0.30277109146118164),\n",
       " ('ㅎㅓㄴㅌㅣㅇㅋㅐㅂ', 0.2951756417751312),\n",
       " ('ㅅㅡ-ㅌㅗㄱㅎㅗㄹㄹㅡㅁ', 0.2912899851799011),\n",
       " ('ㅌㅓ-ㅂㅜㄹ', 0.28863829374313354),\n",
       " ('ㅅㅏㄴㅂㅏㅌ', 0.2860708236694336),\n",
       " ('ㅍㅣ-ㅍㅡ-ㅌㅣㄴ', 0.2818150222301483),\n",
       " ('ㄹㅏ-ㅌㅜ-ㄹㅡ-', 0.2814582288265228),\n",
       " ('ㅂㅣ-ㅈㅔㄴ', 0.2802598774433136),\n",
       " ('ㅅㅏㄴㅂㅏㄹ', 0.27986812591552734)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.wv.most_similar('코로나', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nouns, skipgram_nouns, m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 844013/844013 [00:16<00:00, 49667.44it/s]\n"
     ]
    }
   ],
   "source": [
    "nouns=[]\n",
    "with open('corpus_nouns.txt', 'r', encoding='utf-8') as f:\n",
    "    for sentence in tqdm(f.readlines()):\n",
    "        nouns.append(sentence.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_doc_morphs = pd.Series(morphs)\n",
    "tokenized_doc_nouns = pd.Series(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary_morphs = corpora.Dictionary(tokenized_doc_morphs)\n",
    "dictionary_nouns = corpora.Dictionary(tokenized_doc_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save\n",
    "# np.save('/storage/rok/Korean_Embedding/LSA_dict_morphs.npy', dictionary_morphs.token2id)\n",
    "np.save('LSA_dict_nouns.npy', dictionary_nouns.token2id)\n",
    "\n",
    "# Load\n",
    "# d1 = np.load('/storage/rok/Korean_Embedding/LSA_dict_morphs.npy',allow_pickle='TRUE').item()\n",
    "d2 = np.load('LSA_dict_nouns.npy',allow_pickle='TRUE').item()\n",
    "\n",
    "# print(d1['사랑']) # displays \"world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.220*\"코로나\" + 0.151*\"확진\" + 0.149*\"접종\" + 0.143*\"대표\" + 0.135*\"국민\" + 0.131*\"기업\" + 0.129*\"서울\" + 0.127*\"정부\" + 0.127*\"백신\" + 0.125*\"지역\"'),\n",
       " (1,\n",
       "  '-0.444*\"접종\" + -0.360*\"확진\" + -0.329*\"백신\" + -0.267*\"코로나\" + 0.173*\"대표\" + -0.136*\"감염\" + 0.136*\"기업\" + -0.131*\"방역\" + -0.121*\"수도\" + 0.115*\"국민\"'),\n",
       " (2,\n",
       "  '-0.297*\"국민\" + 0.219*\"기업\" + -0.219*\"후보\" + 0.214*\"사업\" + -0.213*\"총장\" + -0.203*\"대표\" + -0.169*\"의원\" + -0.160*\"지사\" + -0.142*\"대선\" + -0.141*\"대통령\"'),\n",
       " (3,\n",
       "  '0.540*\"접종\" + -0.426*\"확진\" + 0.410*\"백신\" + 0.135*\"예약\" + -0.131*\"올림픽\" + -0.129*\"수도\" + -0.111*\"코로나\" + -0.109*\"단계\" + -0.107*\"검사\" + -0.103*\"서울\"'),\n",
       " (4,\n",
       "  '-0.495*\"올림픽\" + -0.330*\"도쿄\" + -0.281*\"선수\" + -0.255*\"일본\" + -0.232*\"한국\" + -0.141*\"경기\" + 0.122*\"국민\" + -0.121*\"중국\" + -0.111*\"미국\" + -0.109*\"여자\"'),\n",
       " (5,\n",
       "  '-0.232*\"지원\" + 0.217*\"투자\" + 0.213*\"금융\" + -0.210*\"사업\" + 0.193*\"분기\" + -0.166*\"지역\" + -0.150*\"교육\" + 0.150*\"시장\" + 0.137*\"대비\" + 0.134*\"증가\"'),\n",
       " (6,\n",
       "  '0.280*\"수사\" + -0.237*\"국민\" + 0.217*\"사건\" + -0.178*\"소득\" + 0.177*\"검찰\" + -0.165*\"정부\" + 0.163*\"검사\" + -0.147*\"지원\" + -0.144*\"지원금\" + 0.141*\"혐의\"'),\n",
       " (7,\n",
       "  '-0.302*\"확진\" + -0.232*\"후보\" + -0.217*\"대표\" + -0.193*\"사업\" + 0.191*\"정부\" + -0.167*\"기업\" + 0.156*\"수사\" + -0.152*\"지사\" + 0.147*\"가구\" + 0.140*\"소득\"'),\n",
       " (8,\n",
       "  '-0.380*\"중국\" + -0.203*\"미국\" + 0.198*\"대표\" + -0.183*\"정부\" + -0.176*\"대통령\" + -0.160*\"기업\" + -0.157*\"경제\" + 0.152*\"금융\" + -0.145*\"일본\" + -0.127*\"국가\"'),\n",
       " (9,\n",
       "  '-0.208*\"사업\" + 0.208*\"사람\" + -0.206*\"총장\" + 0.194*\"영화\" + -0.187*\"수사\" + -0.167*\"검찰\" + 0.155*\"우리\" + 0.155*\"생각\" + -0.136*\"올림픽\" + -0.133*\"사건\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix_nouns = [dictionary_nouns.doc2bow(doc) for doc in tokenized_doc_nouns]\n",
    "lsamodel_nouns = LsiModel(doc_term_matrix_nouns, num_topics=10, id2word = dictionary_nouns)  # train model\n",
    "lsamodel_nouns.print_topics(num_topics=10, num_words=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 밑에 틀렷으니 다시 실행만하면됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsamodel_nouns.save('LSA_nouns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del  lsamodel_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
